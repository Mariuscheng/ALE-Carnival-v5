{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(\"ALE/Carnival-v5\", full_action_space=True)\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP',\n",
       " 'FIRE',\n",
       " 'UP',\n",
       " 'RIGHT',\n",
       " 'LEFT',\n",
       " 'DOWN',\n",
       " 'UPRIGHT',\n",
       " 'UPLEFT',\n",
       " 'DOWNRIGHT',\n",
       " 'DOWNLEFT',\n",
       " 'UPFIRE',\n",
       " 'RIGHTFIRE',\n",
       " 'LEFTFIRE',\n",
       " 'DOWNFIRE',\n",
       " 'UPRIGHTFIRE',\n",
       " 'UPLEFTFIRE',\n",
       " 'DOWNRIGHTFIRE',\n",
       " 'DOWNLEFTFIRE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {'lives': 0,\n",
       "  'episode_frame_number': 0,\n",
       "  'frame_number': 0,\n",
       "  'seeds': (693650678, 2973253328)})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, info = env.reset(seed=123, options={})\n",
    "observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lives': 0, 'episode_frame_number': 4, 'frame_number': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the preprocessing function moves the tensor to the correct device\n",
    "def preprocess_observation(obs):\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((84, 84)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    return transform(obs).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 100000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "memory_size = 10000\n",
    "target_update_frequency = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize DQN and target network\n",
    "policy_net = DQN(action_space).to(device)\n",
    "target_net = DQN(action_space).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, steps_done):\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n",
    "              np.exp(-1. * steps_done / epsilon_decay)\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].item()\n",
    "    else:\n",
    "        return random.randrange(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = zip(*transitions)\n",
    "\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    states = torch.cat(states).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "    next_states = torch.cat(next_states).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "\n",
    "    state_action_values = policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "    next_state_values = target_net(next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * gamma * (1 - dones)) + rewards\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward: 960.0\n",
      "Episode 2: Total Reward: 680.0\n",
      "Episode 3: Total Reward: 240.0\n",
      "Episode 4: Total Reward: 480.0\n",
      "Episode 5: Total Reward: 880.0\n",
      "Episode 6: Total Reward: 700.0\n",
      "Episode 7: Total Reward: 820.0\n",
      "Episode 8: Total Reward: 500.0\n",
      "Episode 9: Total Reward: 280.0\n",
      "Episode 10: Total Reward: 740.0\n",
      "Episode 11: Total Reward: 800.0\n",
      "Episode 12: Total Reward: 820.0\n",
      "Episode 13: Total Reward: 1180.0\n",
      "Episode 14: Total Reward: 440.0\n",
      "Episode 15: Total Reward: 280.0\n",
      "Episode 16: Total Reward: 840.0\n",
      "Episode 17: Total Reward: 1480.0\n",
      "Episode 18: Total Reward: 720.0\n",
      "Episode 19: Total Reward: 540.0\n",
      "Episode 20: Total Reward: 860.0\n",
      "Episode 21: Total Reward: 420.0\n",
      "Episode 22: Total Reward: 680.0\n",
      "Episode 23: Total Reward: 480.0\n",
      "Episode 24: Total Reward: 740.0\n",
      "Episode 25: Total Reward: 740.0\n",
      "Episode 26: Total Reward: 1900.0\n",
      "Episode 27: Total Reward: 280.0\n",
      "Episode 28: Total Reward: 940.0\n",
      "Episode 29: Total Reward: 1060.0\n",
      "Episode 30: Total Reward: 900.0\n",
      "Episode 31: Total Reward: 900.0\n",
      "Episode 32: Total Reward: 420.0\n",
      "Episode 33: Total Reward: 1340.0\n",
      "Episode 34: Total Reward: 480.0\n",
      "Episode 35: Total Reward: 1220.0\n",
      "Episode 36: Total Reward: 2100.0\n",
      "Episode 37: Total Reward: 480.0\n",
      "Episode 38: Total Reward: 640.0\n",
      "Episode 39: Total Reward: 840.0\n",
      "Episode 40: Total Reward: 580.0\n",
      "Episode 41: Total Reward: 740.0\n",
      "Episode 42: Total Reward: 700.0\n",
      "Episode 43: Total Reward: 940.0\n",
      "Episode 44: Total Reward: 640.0\n",
      "Episode 45: Total Reward: 740.0\n",
      "Episode 46: Total Reward: 740.0\n",
      "Episode 47: Total Reward: 340.0\n",
      "Episode 48: Total Reward: 1060.0\n",
      "Episode 49: Total Reward: 600.0\n",
      "Episode 50: Total Reward: 880.0\n",
      "Episode 51: Total Reward: 420.0\n",
      "Episode 52: Total Reward: 760.0\n",
      "Episode 53: Total Reward: 960.0\n",
      "Episode 54: Total Reward: 660.0\n",
      "Episode 55: Total Reward: 700.0\n",
      "Episode 56: Total Reward: 1480.0\n",
      "Episode 57: Total Reward: 780.0\n",
      "Episode 58: Total Reward: 580.0\n",
      "Episode 59: Total Reward: 1440.0\n",
      "Episode 60: Total Reward: 740.0\n",
      "Episode 61: Total Reward: 540.0\n",
      "Episode 62: Total Reward: 440.0\n",
      "Episode 63: Total Reward: 860.0\n",
      "Episode 64: Total Reward: 1120.0\n",
      "Episode 65: Total Reward: 780.0\n",
      "Episode 66: Total Reward: 840.0\n",
      "Episode 67: Total Reward: 680.0\n",
      "Episode 68: Total Reward: 960.0\n",
      "Episode 69: Total Reward: 1660.0\n",
      "Episode 70: Total Reward: 1240.0\n",
      "Episode 71: Total Reward: 1000.0\n",
      "Episode 72: Total Reward: 760.0\n",
      "Episode 73: Total Reward: 520.0\n",
      "Episode 74: Total Reward: 880.0\n",
      "Episode 75: Total Reward: 840.0\n",
      "Episode 76: Total Reward: 500.0\n",
      "Episode 77: Total Reward: 900.0\n",
      "Episode 78: Total Reward: 460.0\n",
      "Episode 79: Total Reward: 800.0\n",
      "Episode 80: Total Reward: 940.0\n",
      "Episode 81: Total Reward: 800.0\n",
      "Episode 82: Total Reward: 1120.0\n",
      "Episode 83: Total Reward: 1660.0\n",
      "Episode 84: Total Reward: 700.0\n",
      "Episode 85: Total Reward: 1220.0\n",
      "Episode 86: Total Reward: 880.0\n",
      "Episode 87: Total Reward: 1180.0\n",
      "Episode 88: Total Reward: 420.0\n",
      "Episode 89: Total Reward: 940.0\n",
      "Episode 90: Total Reward: 1440.0\n",
      "Episode 91: Total Reward: 700.0\n",
      "Episode 92: Total Reward: 780.0\n",
      "Episode 93: Total Reward: 920.0\n",
      "Episode 94: Total Reward: 920.0\n",
      "Episode 95: Total Reward: 740.0\n",
      "Episode 96: Total Reward: 420.0\n",
      "Episode 97: Total Reward: 840.0\n",
      "Episode 98: Total Reward: 540.0\n",
      "Episode 99: Total Reward: 1380.0\n",
      "Episode 100: Total Reward: 920.0\n",
      "Episode 101: Total Reward: 500.0\n",
      "Episode 102: Total Reward: 1100.0\n",
      "Episode 103: Total Reward: 940.0\n",
      "Episode 104: Total Reward: 840.0\n",
      "Episode 105: Total Reward: 980.0\n",
      "Episode 106: Total Reward: 1080.0\n",
      "Episode 107: Total Reward: 1000.0\n",
      "Episode 108: Total Reward: 1040.0\n",
      "Episode 109: Total Reward: 800.0\n",
      "Episode 110: Total Reward: 620.0\n",
      "Episode 111: Total Reward: 460.0\n",
      "Episode 112: Total Reward: 1020.0\n",
      "Episode 113: Total Reward: 920.0\n",
      "Episode 114: Total Reward: 740.0\n",
      "Episode 115: Total Reward: 680.0\n",
      "Episode 116: Total Reward: 1000.0\n",
      "Episode 117: Total Reward: 960.0\n",
      "Episode 118: Total Reward: 180.0\n",
      "Episode 119: Total Reward: 1040.0\n",
      "Episode 120: Total Reward: 560.0\n",
      "Episode 121: Total Reward: 1520.0\n",
      "Episode 122: Total Reward: 1280.0\n",
      "Episode 123: Total Reward: 960.0\n",
      "Episode 124: Total Reward: 760.0\n",
      "Episode 125: Total Reward: 1040.0\n",
      "Episode 126: Total Reward: 460.0\n",
      "Episode 127: Total Reward: 720.0\n",
      "Episode 128: Total Reward: 460.0\n",
      "Episode 129: Total Reward: 220.0\n",
      "Episode 130: Total Reward: 840.0\n",
      "Episode 131: Total Reward: 900.0\n",
      "Episode 132: Total Reward: 500.0\n",
      "Episode 133: Total Reward: 1040.0\n",
      "Episode 134: Total Reward: 360.0\n",
      "Episode 135: Total Reward: 800.0\n",
      "Episode 136: Total Reward: 1100.0\n",
      "Episode 137: Total Reward: 520.0\n",
      "Episode 138: Total Reward: 540.0\n",
      "Episode 139: Total Reward: 980.0\n",
      "Episode 140: Total Reward: 1860.0\n",
      "Episode 141: Total Reward: 1220.0\n",
      "Episode 142: Total Reward: 500.0\n",
      "Episode 143: Total Reward: 1340.0\n",
      "Episode 144: Total Reward: 940.0\n",
      "Episode 145: Total Reward: 1040.0\n",
      "Episode 146: Total Reward: 1100.0\n",
      "Episode 147: Total Reward: 900.0\n",
      "Episode 148: Total Reward: 700.0\n",
      "Episode 149: Total Reward: 280.0\n",
      "Episode 150: Total Reward: 980.0\n",
      "Episode 151: Total Reward: 900.0\n",
      "Episode 152: Total Reward: 700.0\n",
      "Episode 153: Total Reward: 640.0\n",
      "Episode 154: Total Reward: 680.0\n",
      "Episode 155: Total Reward: 1040.0\n",
      "Episode 156: Total Reward: 680.0\n",
      "Episode 157: Total Reward: 400.0\n",
      "Episode 158: Total Reward: 740.0\n",
      "Episode 159: Total Reward: 700.0\n",
      "Episode 160: Total Reward: 580.0\n",
      "Episode 161: Total Reward: 740.0\n",
      "Episode 162: Total Reward: 420.0\n",
      "Episode 163: Total Reward: 1000.0\n",
      "Episode 164: Total Reward: 500.0\n",
      "Episode 165: Total Reward: 380.0\n",
      "Episode 166: Total Reward: 1300.0\n",
      "Episode 167: Total Reward: 1060.0\n",
      "Episode 168: Total Reward: 940.0\n",
      "Episode 169: Total Reward: 960.0\n",
      "Episode 170: Total Reward: 1440.0\n",
      "Episode 171: Total Reward: 420.0\n",
      "Episode 172: Total Reward: 1120.0\n",
      "Episode 173: Total Reward: 680.0\n",
      "Episode 174: Total Reward: 500.0\n",
      "Episode 175: Total Reward: 1040.0\n",
      "Episode 176: Total Reward: 1120.0\n",
      "Episode 177: Total Reward: 1120.0\n",
      "Episode 178: Total Reward: 660.0\n",
      "Episode 179: Total Reward: 840.0\n",
      "Episode 180: Total Reward: 2400.0\n",
      "Episode 181: Total Reward: 1060.0\n",
      "Episode 182: Total Reward: 780.0\n",
      "Episode 183: Total Reward: 1480.0\n",
      "Episode 184: Total Reward: 220.0\n",
      "Episode 185: Total Reward: 1160.0\n",
      "Episode 186: Total Reward: 260.0\n",
      "Episode 187: Total Reward: 1960.0\n",
      "Episode 188: Total Reward: 440.0\n",
      "Episode 189: Total Reward: 760.0\n",
      "Episode 190: Total Reward: 340.0\n",
      "Episode 191: Total Reward: 180.0\n",
      "Episode 192: Total Reward: 1420.0\n",
      "Episode 193: Total Reward: 180.0\n",
      "Episode 194: Total Reward: 860.0\n",
      "Episode 195: Total Reward: 940.0\n",
      "Episode 196: Total Reward: 1040.0\n",
      "Episode 197: Total Reward: 1000.0\n",
      "Episode 198: Total Reward: 680.0\n",
      "Episode 199: Total Reward: 1720.0\n",
      "Episode 200: Total Reward: 700.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 200\n",
    "steps_done = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    state = preprocess_observation(obs)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, steps_done)\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = preprocess_observation(next_obs)\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "        steps_done += 1\n",
    "\n",
    "        if steps_done % target_update_frequency == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at saved_models\\dqn_carnival.pth\n"
     ]
    }
   ],
   "source": [
    "# Add this import at the top\n",
    "import os\n",
    "\n",
    "# Save the model\n",
    "model_dir = 'saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'dqn_carnival.pth')\n",
    "\n",
    "torch.save(policy_net.state_dict(), model_path)\n",
    "print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gameplay saved as carnival_gameplay.gif\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import imageio\n",
    "\n",
    "# Define your DQN model here or import it\n",
    "# from DQN import DQN  \n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'saved_models/dqn_carnival.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy_net = DQN(action_space).to(device)\n",
    "policy_net.load_state_dict(torch.load(model_path, map_location=device, weights_only=False))\n",
    "policy_net.eval()\n",
    "\n",
    "# Initialize environment with rgb_array mode\n",
    "env = gym.make(\"ALE/Carnival-v5\", full_action_space=True, render_mode=\"rgb_array\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_observation(obs):\n",
    "    transform = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Grayscale(),\n",
    "        T.Resize((84, 84)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    return transform(obs).unsqueeze(0).to(device)\n",
    "\n",
    "# Play one episode and save frames\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "frames = []\n",
    "\n",
    "while not done:\n",
    "    state = preprocess_observation(obs)\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state).max(1)[1].item()\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Capture the current frame\n",
    "    frame = env.render()  # This will now return an RGB array\n",
    "    frames.append(frame)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save frames as a gif\n",
    "gif_path = 'carnival_gameplay.gif'\n",
    "imageio.mimsave(gif_path, frames, fps=30)\n",
    "\n",
    "print(f\"Gameplay saved as {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
